% -*- root: ../supcom.tex -*-

\section{Exam 2012} % (fold)
\label{sec:exam_2012}

\subsection{Problem 1} % (fold)
\label{sub:problem_1}


\begin{question}
  Consider the solution of the linear system of equations
  \begin{equation}
    \mathbf{Ax} = \mathbf{b} \label{eq:3dpoissonmat}
  \end{equation}
  where $\mathbf{A}$ is a $(n-1)^3 \times (n-1)^3$ matrix and $\mathbf{b}$ is a vector of length $(n-1)^3$. Here $\mathbf{A}$ originates from the discretization of a $3D$ Poisson problem with homogenous Dirichet boundary conditions, using a centered difference approach on a grid with $n+1$ grid points in each direction; see Figure~\ref{fig:3d-stencil}. The unknowns are numbered in a natural order running fastest along the $x$ direction. For implicity we also introduce number of degrees of freedom in each spatial direction;
  \begin{equation}
    m = n-1 \label{eq:3dpoisson_degrees_of_freedom}
  \end{equation}
  The resulting matrix will have a bandwidth
  \begin{equation}
    b = m^2 \label{eq:3dpoisson_bandwidth}
  \end{equation}

  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{illustrations/stencils/3d-center.pdf}
    \caption{The stencil used to form \eqref{eq:3dpoissonmat}.}
    \label{fig:3d-stencil}
  \end{figure}
\end{question}


\subsubsection{Memory requirement: LU-decomposition} % (fold)
\label{ssub:memory_requirement_lu_decomposition}


\begin{question}
  Let $n=30$. Assuming we use dense matrices and full LU-decomposition, what is the approximate memory requirement? All numbers are stored in double precision.
\end{question}
We need to store 3 full matrices, $\mathbf{A}$, $\mathbf{L}$ and $\mathbf{U}$, the \emph{load vector} $\mathbf{b}$, as well as solution vector $\mathbf{x}$. All the three full matrices are of size
\[
  M_A = M_L = M_U = (n-3)^3\times (n-1)^3 = m^3\times m^3 = m^6
\]
while the vectors are of size
\[
  M_b = M_x = (n-1)^3 = m^3
\]
so the total memory requirement (given that all numbers are doubles) is
\begin{equation}
  M_{tot} = \left( 3m^6 + 2m^3 \right) \times 8 \approx 14 \mathrm{GB}
\end{equation}
% subsubsection memory_requirement_lu_decomposition (end)


\subsubsection{Memory requirement: Banded Cholesky} % (fold)
\label{ssub:memory_requirement_banded_cholesky}


\begin{question}
  If we instead use a banded Cholesky approach, what is the expected memory usage?
\end{question}
% subsubsection memory_requirement_banded_cholesky (end)


\subsubsection{FLOP per iteration} % (fold)
\label{ssub:flop_per_iteration}


\begin{question}
  Give an estimate of the required number of FLOP (per iteration) if we employ a preconditioned conjugate gradient method. You can assume that we use a solver package that gives us access to a very efficient, diagonal preconditioner, which requires 1 flop/unknown. You can ignore any boundary effects.
\end{question}
% subsubsection flop_per_iteration (end)


\subsubsection{Parallel computation time} % (fold)
\label{ssub:parallel_computation_time}

\begin{question}
  The machine we have access to to solve \eqref{eq:3dpoissonmat} has a slow network on \emph{only allows for a pure distributed memory model}. You can assume a simple network model where the time to send $k$ bytes over the network, $\tau_c(k)$ can be estimated as
  \begin{equation}
    \tau_c(k) = \tau_s + \gamma \cdot k
  \end{equation}
  where $\tau_s$ is a startup time and $\gamma$ is the inverse bandwidth.
  \[
    \tau_s = 10^{-4}\mathrm{s} \; \gamma = 10^{-7}\mathrm{s/byte}
  \]

  We also assume a very simple model for the computation times, and assume that we get 50\% of the full superscalar behaviour of the processors, which runs at 2.4GHz.

  The time restriction is \emph{10 seconds}, and we can use \emph{64 processors}. The algorithm in use is the preconditioned conjugate gradient (CG) method mentioned earlier, in combination with domain decomposition.

  Using the diagonal preconditioner, we need 10 iterations to reach an acceptable tolerance. You can still ignore any boundary effects.

  What is the largest problem size we can run?
\end{question}

% subsubsection parallel_computation_time (end)

% subsection problem_1 (end)





\clearpage
\subsection{Problem 2} % (fold)
\label{sub:problem_2}


\begin{question}
  Assume that you are given $\mathbf{L}$, which is some general, full matrix (no sparsity can be expected) of dimension $N\times N$. We are then asked to perform the matrix-vector product
  \begin{equation}
    \mathbf{u} = \mathbf{Lx}
  \end{equation}
  for some (given) vector x.
\end{question}

\subsubsection{Partitioning} % (fold)
\label{ssub:partitioning}


\begin{question}
  Outline the different ways we can partition the matrix in a distributed memory setting.
\end{question}

\noindent There are two basic approaces: either strips (rows or columns) or blocks.

\begin{center}
  \includegraphics[width=\textwidth]{illustrations/io/splitting.pdf}
\end{center}
% subsubsection partitioning (end)


\subsubsection{Parallel computation of matrix-vector product} % (fold)
\label{ssub:parallel_computation_of_matrix_vector_product}


\begin{question}
  Outline how you would perform the matrix-matrix product for the different partitioning approaches. Which of these methods do you think is the most efficient? Consider both the number of FLOP required and the communication time (you can use the network model from task 1). Is any of the partitioning approaches peferable with a hybrid model?

  \noindent Communication model:
  \begin{itemize} \itemsep=0em
    \item Time to send $k$ bytes: $\tau_c(k) = \tau_s + \gamma \cdot k$
    \item Startup time: $\tau_s = 10^{-4}$s
    \item Inverse bandwidth: $\gamma = 10^{-7}$s/byte
  \end{itemize}

\end{question}

\begin{itemize}
  \item Gathering the result vector on all processes at the end.
  \item Not taking into account how the different parts of $L$ and $x$ get to their respective processes, as this can be done in various ways: each process reading from disk; each process generates it from some functon, etc.
\end{itemize}

The searial computation requires $n$ inner products, each of which requires $(n-1)$ additions and $n$ multiplications:
\begin{equation}
  T_1 = n((n-1)+n) \approx 2n^2
\end{equation}

\noindent\textbf{Row-based strip partitioning:}

\begin{center}
  \includegraphics[]{illustrations/partitioning/strip-row.pdf}
\end{center}

\emph{Computation time:} Each performs $T_1/P$ work:
\[
  T_p^r(calc) = \frac{T_1}{P} = \frac{2n^2}{P} \tau_F
\]

\emph{Communication time:} The only communication needed is for gathering the solution vector at the end. This can be done using MPI\_Allreduce, which requires $\log_2 P$ steps, where $n/P$ data is sent in the first step, $2\cdot n/P$ in the second, etc.
\[
  T_p^r(comm) = \sum_{i=1}^{\log_2 P} \left( \tau_S + 2^{i-1}w \frac{n}{p} \gamma \right)
  = \tau_S \log_2 P + w \frac{n}{p} \left( P-1 \right) \gamma
\]

\emph{Total time:}
\[
  T_p^r (total) = T_p^r(calc) + T_p^r(comm) =  \frac{2n^2}{P} \tau_F + \tau_S \log_2 P + w \frac{n}{p} \left( 2^{\log_2 P}-1 \right) \gamma
\]

\emph{Speedup:}
% TODO









\noindent\textbf{Column-based strip partitioning:}

\begin{center}
  \includegraphics[]{illustrations/partitioning/strip-col.pdf}
\end{center}

\emph{Computation time:} We need to compute all the parts of the result vector and them sum up the parts from all the partitions to get the final result:
\[
  T_p^c(calc) = \left[ \frac{n}{P} \left( \frac{n}{P} - 1 + \frac{n}{P} \right) \cdot P + \frac{n}{p}\cdot P \right] \tau_F
  = \left[ n \left( 2 \frac{n}{P} - 1 \right) + n \right] \cdot \tau_F
\]

\emph{Communication time:} The communication we need to do is to sum all the parts of the result vector computed in each process. This can be done using MPI\_Allreduce with the sum option to end up with the complete result vector on all processes in $\log_2 P$ sends, each consisting of $n$ elements:
\begin{equation}
  T_p^c(comm) = \log_2 P \left( \tau_S + wn\gamma \right)
\end{equation}

\emph{Total time:}
\[
  T_p^c (total) = \left[ n \left( 2 \frac{n}{P} - 1 \right) + n \right] \cdot \tau_F + \log_2 P \left( \tau_S + wn\gamma \right)
\]






\noindent\textbf{Block-based partitioning:}

\emph{Computation time:}
\[
  T_p^b (calc) = \left[ \frac{n}{\sqrt{P}} \left( 2 \frac{n}{\sqrt{P}} - 1 \right) + n \right] \tau_F
\]

\emph{Communication time:} The communication needed is for the MPI\_Allreduce to get the final result vector on each row of blocks, followed by an MPI\_Allgather to get the complete result vector on all processes:
\[
  T_p^b(comm) = \log_2 \sqrt{P} \left( \tau_S \frac{n}{\sqrt{P}}w\gamma \right)
  + \tau_S \log_2 \sqrt{P} + w \frac{n}{\sqrt{P}} \left(\sqrt{P}-1 \right) \gamma
\]

\emph{Total time:}
\[
  T_p^b (total) =  \left[ \frac{n}{\sqrt{P}} \left( 2 \frac{n}{\sqrt{P}} - 1 \right) + n \right] \tau_F +  \log_2 \sqrt{P} \left( \tau_S \frac{n}{\sqrt{P}}w\gamma \right)
  + \tau_S \log_2 \sqrt{P} + w \frac{n}{\sqrt{P}} \left(\sqrt{P}-1 \right) \gamma
\]

\begin{center}
  \includegraphics[]{illustrations/partitioning/block.pdf}
\end{center}



\noindent \textbf{Comparison:}
% subsubsection parallel_computation_of_matrix_vector_product (end)





% subsection problem_2 (end)











\clearpage
\subsection{Problem 3: True/False} % (fold)
\label{sub:problem_3}

\begin{question}
  An $n$-way associative cache is much more prone to cache trashing.
\end{question}
\textbf{False.} It is much less prone to cache trashing because all memory locations can be mapped to $n$ different cache lines.

\vskip2em
\begin{question}
  Dense $LU$ factorization is useless for large systems due to the low FLOPS.
\end{question}
\textbf{False.} It gives close to optimal FLOPS. It is unusable for larger systems due to the amount of operations required, not the speed they are performed at.

\vskip2em
\begin{question}
  If your program is compute bound, a shared memory implementation can be faster than a distributed memory implementation.
\end{question}
\textbf{True.} If memory is not an issue a shared memory implementation will be faster, because a distributed memory implementation would incur communication time. It will also benefit from better cache usage.


\vskip2em
\begin{question}
  The conjugate gradient method is a direct solution method.
\end{question}
\textbf{False.} In general it is considered to be an iterative method. Although it may be considered direct if only exact arithmetics are involved, as it will then only require $n$ iterations.

\vskip2em
\begin{question}
  The BLAS and LAPACK libraries generally work with row-major matrix formats.
\end{question}
\textbf{False.} They are implemented in -- and originally designed for -- FORTRAN, which is column-major.

\vskip2em
\begin{question}
  CMake is used to build a program.
\end{question}
\textbf{False.} CMake are used to generate a build system (e.g. makefiles), which are in turn used to build a program.

\vskip2em
\begin{question}
  If we do parallel Monte-Carlo simulations, it is imperative to know the period of the PRNG (Pseudo Random Number Generator) to avoid doing the same simulations several times within a single process.
\end{question}
\textbf{True.} The period tells us how many simulations we can do on a single process before the sequence repeats. The seed is what ensures different simulations are done on the different processes.

\vskip2em
\begin{question}
  The fast diagonalization method considered in the course is optimal in sense of the number of FLOP required. Hint: optimal has a particular meaning in the context!
\end{question}
\textbf{False.} The method is $O(N^2 \log_2 N)$, i.e. we use $\log_2 N$ operations per degree of freedom; $N$ would be optimal.

% subsection problem_3 (end)

% section exam_2012 (end)
